#!/usr/bin/env python3
"""
train_model.py
Random Forest ëª¨ë¸ í›ˆë ¨ (CICIDS2017 ë°ì´í„°ì…‹)
"""

import pandas as pd
import numpy as np
import joblib
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.metrics import classification_report, accuracy_score
import logging
from pathlib import Path

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s'
)

# CICIDS2017 ë°ì´í„°ì…‹ íŒŒì¼ ëª©ë¡
DATASET_FILES = [
    'data/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv',
    'data/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv',
    'data/Friday-WorkingHours-Morning.pcap_ISCX.csv',
    'data/Monday-WorkingHours.pcap_ISCX.csv',
    'data/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv',
    'data/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv',
    'data/Tuesday-WorkingHours.pcap_ISCX.csv',
    'data/Wednesday-workingHours.pcap_ISCX.csv'
]

# ì¶œë ¥ ë””ë ‰í† ë¦¬
OUTPUT_DIR = Path('models')


def load_and_clean_data(file_list):
    """
    ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬
    
    Args:
        file_list: CSV íŒŒì¼ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        X (DataFrame): Feature
        y (Series): Label
    """
    logging.info("=" * 60)
    logging.info("ë°ì´í„° ë¡œë“œ ì‹œì‘")
    logging.info("=" * 60)
    
    dfs = []
    
    for file_path in file_list:
        if not Path(file_path).exists():
            logging.warning(f"íŒŒì¼ ì—†ìŒ: {file_path}")
            continue
        
        logging.info(f"ë¡œë“œ ì¤‘: {file_path}")
        df = pd.read_csv(file_path, encoding='utf-8', low_memory=False)
        dfs.append(df)
    
    if not dfs:
        raise FileNotFoundError("ë°ì´í„°ì…‹ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
    
    # ëª¨ë“  ë°ì´í„° í†µí•©
    data = pd.concat(dfs, ignore_index=True)
    logging.info(f"âœ“ ì´ {len(data):,}ê°œ í–‰ ë¡œë“œë¨")
    
    # ì»¬ëŸ¼ ì´ë¦„ ì •ë¦¬ (ê³µë°± ì œê±°, ì–¸ë”ìŠ¤ì½”ì–´ ë³€í™˜)
    data.columns = data.columns.str.strip().str.replace(' ', '_')
    
    logging.info("\në°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
    
    # ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°
    drop_columns = ['Flow_ID', 'Source_IP', 'Source_Port', 
                    'Destination_IP', 'Destination_Port', 'Timestamp']
    
    for col in drop_columns:
        if col in data.columns:
            data = data.drop(columns=[col])
    
    # ë¬´í•œëŒ€ ê°’ì„ NaNìœ¼ë¡œ ë³€í™˜
    data = data.replace([np.inf, -np.inf], np.nan)
    
    # ê²°ì¸¡ì¹˜ ì œê±°
    before_dropna = len(data)
    data = data.dropna()
    after_dropna = len(data)
    logging.info(f"âœ“ ê²°ì¸¡ì¹˜ ì œê±°: {before_dropna - after_dropna:,}ê°œ í–‰")
    
    # Label ì»¬ëŸ¼ í™•ì¸
    if 'Label' not in data.columns:
        raise ValueError("Label ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤!")
    
    # Feature(X)ì™€ Label(y) ë¶„ë¦¬
    X = data.drop(columns=['Label'])
    y = data['Label']
    
    logging.info(f"âœ“ Feature ê°œìˆ˜: {X.shape[1]}")
    logging.info(f"âœ“ ìµœì¢… ë°ì´í„°: {len(X):,}ê°œ í–‰")
    
    return X, y


def preprocess_features_labels(X, y):
    """
    Feature ìŠ¤ì¼€ì¼ë§ ë° Label ì¸ì½”ë”©
    
    Args:
        X (DataFrame): Feature
        y (Series): Label
    
    Returns:
        X_scaled (ndarray): ìŠ¤ì¼€ì¼ë§ëœ Feature
        y_encoded (ndarray): ì¸ì½”ë”©ëœ Label
        scaler (MinMaxScaler): ìŠ¤ì¼€ì¼ëŸ¬ ê°ì²´
        le (LabelEncoder): ë ˆì´ë¸” ì¸ì½”ë” ê°ì²´
        feature_names (list): Feature ì´ë¦„ ë¦¬ìŠ¤íŠ¸
    """
    logging.info("\n" + "=" * 60)
    logging.info("Feature ì „ì²˜ë¦¬")
    logging.info("=" * 60)
    
    # Feature ì´ë¦„ ì €ì¥
    feature_names = X.columns.tolist()
    logging.info(f"Feature ê°œìˆ˜: {len(feature_names)}")
    
    # Label í†µí•© (ë¹„ìŠ·í•œ ê³µê²© ìœ í˜• í•©ì¹˜ê¸°)
    label_mapping = {
        'Web Attack ï¿½ Brute Force': 'Web Attack',
        'Web Attack ï¿½ XSS': 'Web Attack',
        'Web Attack ï¿½ Sql Injection': 'Web Attack',
    }
    
    y = y.replace(label_mapping)
    
    # Label ì¸ì½”ë”©
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)
    
    logging.info(f"\nê³µê²© ìœ í˜• ({len(le.classes_)}ê°œ):")
    for idx, label in enumerate(le.classes_):
        count = (y == label).sum()
        logging.info(f"  {idx}: {label} ({count:,}ê°œ)")
    
    # Feature ìŠ¤ì¼€ì¼ë§ (0~1 ë²”ìœ„)
    logging.info("\nFeature ìŠ¤ì¼€ì¼ë§ ì¤‘...")
    scaler = MinMaxScaler()
    X_scaled = scaler.fit_transform(X)
    
    logging.info("âœ“ ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ")
    
    return X_scaled, y_encoded, scaler, le, feature_names


def train_and_save_model(X, y, scaler, le, feature_names):
    """
    Random Forest ëª¨ë¸ í›ˆë ¨ ë° ì €ì¥
    
    Args:
        X (ndarray): Feature
        y (ndarray): Label
        scaler: ìŠ¤ì¼€ì¼ëŸ¬
        le: ë ˆì´ë¸” ì¸ì½”ë”
        feature_names: Feature ì´ë¦„ ë¦¬ìŠ¤íŠ¸
    """
    logging.info("\n" + "=" * 60)
    logging.info("ëª¨ë¸ í›ˆë ¨")
    logging.info("=" * 60)
    
    # Train/Test ë¶„í•  (80:20)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, 
        test_size=0.2, 
        random_state=42,
        stratify=y  # í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€
    )
    
    logging.info(f"Train: {len(X_train):,}ê°œ")
    logging.info(f"Test:  {len(X_test):,}ê°œ")
    
    # Random Forest ëª¨ë¸
    logging.info("\nRandom Forest í›ˆë ¨ ì¤‘...")
    logging.info("  - n_estimators: 100")
    logging.info("  - max_depth: 10")
    logging.info("  - n_jobs: -1 (ëª¨ë“  CPU ì‚¬ìš©)")
    
    model = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        random_state=42,
        n_jobs=-1,  # ëª¨ë“  CPU ì½”ì–´ ì‚¬ìš©
        verbose=1
    )
    
    model.fit(X_train, y_train)
    
    logging.info("âœ“ í›ˆë ¨ ì™„ë£Œ")
    
    # í‰ê°€
    logging.info("\nëª¨ë¸ í‰ê°€ ì¤‘...")
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    
    logging.info(f"\nì •í™•ë„: {accuracy:.4f} ({accuracy*100:.2f}%)")
    
    # ìƒì„¸ ë¦¬í¬íŠ¸
    logging.info("\në¶„ë¥˜ ë¦¬í¬íŠ¸:")
    report = classification_report(
        y_test, y_pred, 
        target_names=le.classes_,
        digits=4
    )
    print(report)
    
    # ëª¨ë¸ ì €ì¥
    logging.info("\n" + "=" * 60)
    logging.info("ëª¨ë¸ ì €ì¥")
    logging.info("=" * 60)
    
    OUTPUT_DIR.mkdir(exist_ok=True)
    
    # 1. Random Forest ëª¨ë¸
    model_path = OUTPUT_DIR / 'random_forest_model.joblib'
    joblib.dump(model, model_path)
    logging.info(f"âœ“ ëª¨ë¸: {model_path}")
    
    # 2. MinMax Scaler
    scaler_path = OUTPUT_DIR / 'min_max_scaler.joblib'
    joblib.dump(scaler, scaler_path)
    logging.info(f"âœ“ ìŠ¤ì¼€ì¼ëŸ¬: {scaler_path}")
    
    # 3. Label Encoder
    encoder_path = OUTPUT_DIR / 'label_encoder.joblib'
    joblib.dump(le, encoder_path)
    logging.info(f"âœ“ ì¸ì½”ë”: {encoder_path}")
    
    # 4. Feature Names
    features_path = OUTPUT_DIR / 'feature_names.joblib'
    joblib.dump(feature_names, features_path)
    logging.info(f"âœ“ Feature: {features_path}")
    
    logging.info("\nâœ… ëª¨ë“  íŒŒì¼ ì €ì¥ ì™„ë£Œ!")
    
    # íŒŒì¼ í¬ê¸° í™•ì¸
    logging.info("\níŒŒì¼ í¬ê¸°:")
    for file_path in OUTPUT_DIR.glob('*.joblib'):
        size_mb = file_path.stat().st_size / (1024 * 1024)
        logging.info(f"  {file_path.name}: {size_mb:.2f} MB")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 60)
    print("ğŸ¤– Random Forest ëª¨ë¸ í›ˆë ¨")
    print("=" * 60)
    print()
    
    try:
        # 1. ë°ì´í„° ë¡œë“œ
        X, y = load_and_clean_data(DATASET_FILES)
        
        # 2. ì „ì²˜ë¦¬
        X_scaled, y_encoded, scaler, le, feature_names = preprocess_features_labels(X, y)
        
        # 3. í›ˆë ¨ ë° ì €ì¥
        train_and_save_model(X_scaled, y_encoded, scaler, le, feature_names)
        
        print("\n" + "=" * 60)
        print("âœ… í›ˆë ¨ ì™„ë£Œ!")
        print("=" * 60)
        print()
        print("ìƒì„±ëœ íŒŒì¼:")
        print("  - models/random_forest_model.joblib")
        print("  - models/min_max_scaler.joblib")
        print("  - models/label_encoder.joblib")
        print("  - models/feature_names.joblib")
        print()
        print("ë‹¤ìŒ ë‹¨ê³„:")
        print("  python flow_receiver.py")
        print()
    
    except Exception as e:
        logging.error(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()